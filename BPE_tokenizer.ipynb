{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONnq20kkvdI2wkep/z+c02"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqFRxCbuLUIZ",
        "outputId": "9c1f09ca-cc92-4770-908c-f17c6561d52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6446\n",
            " From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as \n"
          ]
        }
      ],
      "source": [
        "with open(\"shakespeare.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "YQL6HSSNMwfc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed=re.split(r'([,.:;?_!\"()\\']|\\s)',raw_text)\n",
        "preprocessed=[item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))\n",
        "print(preprocessed[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNQ7axDoL5tI",
        "outputId": "2c692b2a-c371-4566-f545-b11003286018"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1321\n",
            "['From', 'fairest', 'creatures', 'we', 'desire', 'increase', ',', 'That', 'thereby', 'beauty', \"'\", 's', 'rose', 'might', 'never', 'die', ',', 'But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease', ',', 'His', 'tender', 'heir', 'might', 'bear', 'his', 'memory', ':', 'But', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes', ',', 'Feed', \"'\", 'st', 'thy', 'light', \"'\", 's', 'flame', 'with', 'self-substantial', 'fuel', ',', 'Making', 'a', 'famine', 'where', 'abundance', 'lies', ',', 'Thy', 'self', 'thy', 'foe', ',', 'to', 'thy', 'sweet', 'self', 'too', 'cruel', ':', 'Thou', 'that', 'art', 'now', 'the', 'world', \"'\", 's', 'fresh', 'ornament', ',', 'And', 'only', 'herald', 'to', 'the', 'gaudy', 'spring', ',', 'Within', 'thine', 'own', 'bud', 'buriest', 'thy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=sorted(set(preprocessed))\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWCMkscHMBMz",
        "outputId": "2a8d043e-420a-4e09-dcd6-4df2fcc110db"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab={token:i for i,token in enumerate(words)}\n",
        "for i,j in enumerate(vocab.items()):\n",
        "  print(j)\n",
        "  if i>9:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86V9__roNFpC",
        "outputId": "787e1661-aa2c-4236-a04f-d69ee05d3d99"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"'\", 0)\n",
            "('(', 1)\n",
            "(')', 2)\n",
            "(',', 3)\n",
            "('.', 4)\n",
            "(':', 5)\n",
            "(';', 6)\n",
            "('?', 7)\n",
            "('A', 8)\n",
            "('Ah', 9)\n",
            "('And', 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words.extend([\"<|unk|>\"])\n",
        "vocab={token:i for i,token in enumerate(words)}\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHbwl9CKNgY8",
        "outputId": "76591f2e-7caf-46a6-b4f4-6a2c885ad193"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "    token_ids=[self.str_to_int[s] for s in preprocessed]\n",
        "    return token_ids\n",
        "  def decode(self, token_ids):\n",
        "    tokens=[self.int_to_str[i] for i in token_ids]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "uYsn-kQAOAys"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text='Hello how are thee my lad'"
      ],
      "metadata": {
        "id": "K710FCL-6qDx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer(vocab)\n",
        "encode=tokenizer.encode(text)\n",
        "print(encode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qjb2mX96xH6",
        "outputId": "ff3db09f-18b4-45ce-ea49-f2825b92a6f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[544, 268, 102, 458, 336, 544]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode=tokenizer.decode(encode)\n",
        "print(decode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UtxxNwN66I6",
        "outputId": "b24ab479-450a-47c4-b823-fcbbda1937d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<|unk|>', 'how', 'are', 'thee', 'my', '<|unk|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BPE\n",
        "from collections import Counter\n",
        "class BPE:\n",
        "  def __init__(self,corpus,vocab_size,max_iter=None):\n",
        "    self.corpus=corpus\n",
        "    self.vocab_size=vocab_size\n",
        "    self.word_freq=Counter()\n",
        "    self.splits={}\n",
        "    self.merges={}\n",
        "    self.max_iter=max_iter\n"
      ],
      "metadata": {
        "id": "91wvfkrH7N7U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(self):\n",
        "  for document in self.corpus:\n",
        "    words=document.split()\n",
        "    self.word_freq+=Counter(words)\n",
        "\n",
        "  for word in self.word_freq:\n",
        "    self.splits[word]=list(word)+['<\\w>']"
      ],
      "metadata": {
        "id": "rsdEXcCtonfN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pairs_freq(self):\n",
        "  pairs_freq=Counter()\n",
        "  for word,freq in self.word_freq.items():\n",
        "    split=self.splits[word]\n",
        "    for i in range(len(split)-1):\n",
        "      pairs_freq[(split[i],split[i+1])]+=freq\n",
        "  return pairs_freq"
      ],
      "metadata": {
        "id": "jOx2OxAspWh_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  while len(self.merges)<self.vocab_size:\n",
        "    pair_freq=self.get_pairs_freq()\n",
        "    if not pair_freq:\n",
        "      break\n",
        "    pair=max(pair_freq,key=pair_freq.get)\n",
        "    self.update_splits(pair)\n",
        "    self.merges[pair]=pair+pair\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "KwhQi6LoqRY0",
        "outputId": "b33e558c-6403-43ee-b193-a6c1bc236fe2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-816e0bbaf7e6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mpair_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pairs_freq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpair_freq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpair_freq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_splits(self, pair):\n",
        "    lhs, rhs = pair\n",
        "    for word in list(self.splits.keys()):\n",
        "        new_split = []\n",
        "        cursor = 0\n",
        "        while cursor < len(self.splits[word]):\n",
        "            if cursor + 1 < len(self.splits[word]) and \\\n",
        "               self.splits[word][cursor] == lhs and \\\n",
        "               self.splits[word][cursor + 1] == rhs:\n",
        "                new_split.append(lhs + rhs)\n",
        "                cursor += 2\n",
        "            else:\n",
        "                new_split.append(self.splits[word][cursor])\n",
        "                cursor += 1\n",
        "        self.splits[word] = new_split"
      ],
      "metadata": {
        "id": "asJBOr8RrUPN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(self, text):\n",
        "    splits = [list(word) + ['</w>'] for word in text.split()]\n",
        "    for lhs, rhs in self.merges:\n",
        "        for idx, split in enumerate(splits):\n",
        "            new_split = []\n",
        "            cursor = 0\n",
        "            while cursor < len(split):\n",
        "                if cursor + 1 < len(split) and split[cursor] == lhs and split[cursor + 1] == rhs:\n",
        "                    new_split.append(lhs + rhs)\n",
        "                    cursor += 2\n",
        "                else:\n",
        "                    new_split.append(split[cursor])\n",
        "                    cursor += 1\n",
        "            splits[idx] = new_split\n",
        "    return sum(splits, [])"
      ],
      "metadata": {
        "id": "mPFzpI0HrbiD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "class BPE:\n",
        "  def __init__(self,corpus,vocab_size,max_iter=None):\n",
        "    self.corpus=corpus\n",
        "    self.vocab_size=vocab_size\n",
        "    self.word_freq=Counter()\n",
        "    self.splits={}\n",
        "    self.merges={}\n",
        "    self.max_iter=max_iter\n",
        "\n",
        "  def train(self):\n",
        "    for document in self.corpus:\n",
        "      words=document.split()\n",
        "      self.word_freq+=Counter(words)\n",
        "\n",
        "    for word in self.word_freq:\n",
        "      self.splits[word]=list(word)+['<\\w>']\n",
        "\n",
        "\n",
        "  def get_pairs_freq(self):\n",
        "    pairs_freq=Counter()\n",
        "    for word,freq in self.word_freq.items():\n",
        "      split=self.splits[word]\n",
        "      for i in range(len(split)-1):\n",
        "        pairs_freq[(split[i],split[i+1])]+=freq\n",
        "    return pairs_freq\n",
        "\n",
        "    while len(self.merges)<self.vocab_size:\n",
        "      pair_freq=self.get_pairs_freq()\n",
        "      if not pair_freq:\n",
        "        break\n",
        "      pair=max(pair_freq,key=pair_freq.get)\n",
        "      self.update_splits(pair)\n",
        "      self.merges[pair]=pair+pair\n",
        "\n",
        "  def update_splits(self, pair):\n",
        "    lhs, rhs = pair\n",
        "    for word in list(self.splits.keys()):\n",
        "        new_split = []\n",
        "        cursor = 0\n",
        "        while cursor < len(self.splits[word]):\n",
        "            if cursor + 1 < len(self.splits[word]) and \\\n",
        "               self.splits[word][cursor] == lhs and \\\n",
        "               self.splits[word][cursor + 1] == rhs:\n",
        "                new_split.append(lhs + rhs)\n",
        "                cursor += 2\n",
        "            else:\n",
        "                new_split.append(self.splits[word][cursor])\n",
        "                cursor += 1\n",
        "        self.splits[word] = new_split\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    splits = [list(word) + ['</w>'] for word in text.split()]\n",
        "    for lhs, rhs in self.merges:\n",
        "        for idx, split in enumerate(splits):\n",
        "            new_split = []\n",
        "            cursor = 0\n",
        "            while cursor < len(split):\n",
        "                if cursor + 1 < len(split) and split[cursor] == lhs and split[cursor + 1] == rhs:\n",
        "                    new_split.append(lhs + rhs)\n",
        "                    cursor += 2\n",
        "                else:\n",
        "                    new_split.append(split[cursor])\n",
        "                    cursor += 1\n",
        "            splits[idx] = new_split\n",
        "    return sum(splits, [])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "64L1kkfRrjkz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"highest\", \"higher\", \"lower\", \"lowest\", \"cooler\", \"coolest\"]\n",
        "bpe = BPE(corpus, vocab_size=17)\n",
        "bpe.train()\n",
        "\n",
        "sample_text = \"highest higher lower lowest cooler coolest\"\n",
        "tokens = bpe.tokenize(sample_text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edtghqc3sKmj",
        "outputId": "2c6aca20-c50b-4d6d-bbd3-6f3f114abd64"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'i', 'g', 'h', 'e', 's', 't', '</w>', 'h', 'i', 'g', 'h', 'e', 'r', '</w>', 'l', 'o', 'w', 'e', 'r', '</w>', 'l', 'o', 'w', 'e', 's', 't', '</w>', 'c', 'o', 'o', 'l', 'e', 'r', '</w>', 'c', 'o', 'o', 'l', 'e', 's', 't', '</w>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pjwg2qRKsLYb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}